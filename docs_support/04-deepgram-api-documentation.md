# Deepgram API - Documenta√ß√£o Completa para Transcri√ß√£o com Timestamps

## üìå Vis√£o Geral

Deepgram √© uma API de Speech-to-Text (STT) avan√ßada que oferece transcri√ß√£o precisa com timestamps detalhados, identifica√ß√£o de falantes (speaker diarization) e segmenta√ß√£o de utterances. Ideal para criar aplica√ß√µes de voz, legendas em tempo real e an√°lise de conversas.

## üéØ Casos de Uso Principais

### 1. Streaming Audio (√Åudio em Tempo Real)
**Para que serve:**
- Agentes de voz com conversa√ß√£o em tempo real
- Transcri√ß√£o turn-based (baseada em turnos)

**Benef√≠cios:**
- Detec√ß√£o de fim de turno integrada ao modelo
- Din√¢mica de altern√¢ncia configur√°vel

**Exemplos:**
- Contact center agents
- Bots de suporte ao cliente
- Assistentes em tempo real

**In√≠cio r√°pido:** [Flux API Quickstart](https://developers.deepgram.com/docs/flux/quickstart)

**Nota:** Atualmente dispon√≠vel para ingl√™s. Para outras l√≠nguas, use [Streaming API geral](https://developers.deepgram.com/docs/live-streaming-audio).

### 2. Real-time Meeting Transcription
**Para que serve:**
- Transcri√ß√µes em tempo real para reuni√µes e eventos

**Benef√≠cios:**
- Transcritos em tempo real
- Disponibilidade de m√∫ltiplos idiomas
- Transcritos com diarization (identifica√ß√£o de falantes)

**Exemplos:**
- Legendas ao vivo
- Transcri√ß√£o de eventos
- Monitoramento de feeds de √°udio

### 3. Pre-recorded Audio (√Åudio Pr√©-gravado)
**Para que serve:**
- Transcri√ß√£o de arquivos previamente gravados

**Benef√≠cios:**
- Implementa√ß√£o simples
- Ampla disponibilidade de idiomas
- Custo-efetivo

**Exemplos:**
- Transcri√ß√£o de entrevistas
- Podcasts
- Reuni√µes gravadas
- Chamadas de suporte

## üîß Configura√ß√£o B√°sica

### Chamada API Simples (cURL)

```bash
curl -X POST \
  -H "Authorization: Token YOUR_DEEPGRAM_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{"url":"https://static.deepgram.com/examples/Bueller-Life-moves-pretty-fast.wav"}' \
  "https://api.deepgram.com/v1/listen?model=nova-3&language=en&utterances=true&diarize=true&smart_format=true"
```

### Python SDK

```python
from deepgram import DeepgramClient, PrerecordedOptions

# Inicializar cliente
dg = DeepgramClient("YOUR_SECRET")

# URL do √°udio
AUDIO_URL = {
    "url": "https://static.deepgram.com/examples/Bueller-Life-moves-pretty-fast.wav"
}

# Configurar op√ß√µes
opts = PrerecordedOptions(
    model="nova-3",
    language="en",
    smart_format=True,
    utterances=True,
    diarize=True,
)

# Fazer transcri√ß√£o
res = dg.listen.prerecorded.v("1").transcribe_url(AUDIO_URL, opts)

# Imprimir resultado
print(res.to_json(indent=2))
```

## üéõÔ∏è Par√¢metros Principais

### Flags Essenciais

| Flag | Prop√≥sito | Quando Usar |
|------|-----------|-------------|
| `utterances=true` | Retorna `results.utterances[]` com start, end, speaker, transcript, words[] | Sempre que precisar de segmenta√ß√£o contextual |
| `diarize=true` | Adiciona IDs de falante para cada utterance e palavra (√°udio single-channel) | Para conversas com m√∫ltiplos falantes |
| `smart_format=true` | Auto-capitaliza√ß√£o, pontua√ß√£o, normaliza√ß√£o num√©rica | Para transcritos mais leg√≠veis |
| `utt_split` | Ajusta sensibilidade de divis√£o de utterances (ex: `utt_split=1.0`) | Para controlar granularidade de segmenta√ß√£o |

### Modelos Dispon√≠veis

**Nova-2 e Nova-3:**
- Modelos mais recentes
- Suporte completo para utterances e diarization
- Melhor precis√£o

**Escolha do Modelo:**
```python
model="nova-3"  # Recomendado para melhor precis√£o
model="nova-2"  # Alternativa est√°vel
```

## üìä Estrutura da Resposta

### Exemplo de Utterance

```json
{
  "start": 10.0713,
  "end": 11.6713,
  "confidence": 0.9994,
  "channel": 0,
  "transcript": "Life moves pretty fast.",
  "words": [
    {
      "word": "Life",
      "start": 10.0713,
      "end": 10.3113,
      "confidence": 0.9990,
      "speaker": 0
    },
    {
      "word": "moves",
      "start": 10.3113,
      "end": 10.6313,
      "confidence": 0.9997,
      "speaker": 0
    }
  ],
  "speaker": 0,
  "id": "35404fd9-12ed-4b58-a9a9-9eaf93475ef6"
}
```

### Campos Importantes

**N√≠vel de Utterance:**
- `start`: Timestamp de in√≠cio (segundos)
- `end`: Timestamp de fim (segundos)
- `confidence`: Confian√ßa da transcri√ß√£o (0-1)
- `channel`: Canal de √°udio
- `transcript`: Texto completo da utterance
- `speaker`: ID do falante
- `id`: Identificador √∫nico

**N√≠vel de Palavra:**
- `word`: Texto da palavra
- `start`: In√≠cio da palavra (segundos)
- `end`: Fim da palavra (segundos)
- `confidence`: Confian√ßa da palavra
- `speaker`: Falante da palavra

## ‚è±Ô∏è Trabalhando com Timestamps

### Tipos de Timestamps

**1. Word-level Timestamps (N√≠vel de Palavra)**
- Precis√£o m√°xima
- √ötil para sincroniza√ß√£o exata
- Permite highlight palavra por palavra

**2. Utterance-level Timestamps (N√≠vel de Segmento)**
- Segmentos sem√¢nticos completos
- Mais f√°cil de trabalhar
- Bom para navega√ß√£o de conte√∫do

### Casos de Uso por Tipo

| Uso | Timestamp Recomendado | Raz√£o |
|-----|---------------------|--------|
| Legendas | Utterance + Word | Utterance para blocos, word para cueing preciso |
| Busca | Utterance | √çndice por segmento sem√¢ntico |
| Analytics | Utterance | M√©tricas por turno de fala |
| Karaoke-style | Word | Destaque palavra por palavra |

### Exemplo: Calcular Talk Time

```python
from deepgram import DeepgramClient, PrerecordedOptions

def compute_speaking_time(transcript_data):
    """Calcula tempo de fala por speaker"""
    speaker_times = {}
    
    for utterance in transcript_data['results']['utterances']:
        speaker = utterance['speaker']
        duration = utterance['end'] - utterance['start']
        
        if speaker not in speaker_times:
            speaker_times[speaker] = 0
        
        speaker_times[speaker] += duration
    
    return speaker_times

# Uso
dg = DeepgramClient("YOUR_KEY")
response = dg.listen.prerecorded.v("1").transcribe_url(
    {"url": "audio.wav"},
    PrerecordedOptions(
        model="nova-3",
        diarize=True,
        utterances=True
    )
)

times = compute_speaking_time(response.to_dict())
print(f"Speaker times: {times}")
```

## üé§ Speaker Diarization (Identifica√ß√£o de Falantes)

### O Que √â
Processo de identificar "quem falou quando" em uma grava√ß√£o de √°udio com m√∫ltiplos falantes.

### Quando Usar

**Single-channel Audio (√Åudio de Canal √önico):**
```python
PrerecordedOptions(
    diarize=True,  # Ativa diarization
    model="nova-3"
)
```

**Multi-channel Audio (√Åudio Multi-canal):**
```python
PrerecordedOptions(
    channels=2,  # Especifica n√∫mero de canais
    # N√ÉO usar diarize=True
)
```

> **Dica:** Grava√ß√µes de call center geralmente t√™m 2 canais separados (agente e cliente). Nesse caso, use multichannel ao inv√©s de diarization.

### Precis√£o e Limita√ß√µes

**Casos com Alta Precis√£o:**
- Falantes distintos
- Pouca sobreposi√ß√£o
- √Åudio limpo
- Turnos claros

**Casos Desafiadores:**
- Back-channels curtos ("uh-huh", "mm-hmm")
- Fala sobreposta
- Ru√≠do de fundo
- Vozes similares

**Solu√ß√£o para Back-channels:**
```python
# Mesclar utterances muito curtas
def merge_short_utterances(utterances, min_duration=0.5):
    merged = []
    current = None
    
    for utt in utterances:
        duration = utt['end'] - utt['start']
        
        if duration < min_duration and current:
            # Mesclar com anterior
            current['end'] = utt['end']
            current['transcript'] += ' ' + utt['transcript']
        else:
            if current:
                merged.append(current)
            current = utt
    
    if current:
        merged.append(current)
    
    return merged
```

## üìù Gerando Legendas (WebVTT e SRT)

### Usando Helper Library

```python
from deepgram import DeepgramClient, PrerecordedOptions
from deepgram_captions import DeepgramConverter, webvtt, srt

# Transcrever
dg = DeepgramClient()
with open("audio.mp4", "rb") as f:
    payload = f.read()

opts = PrerecordedOptions(
    model="nova-3",
    language="en",
    smart_format=True,
    utterances=True,
    diarize=True,
)

res = dg.listen.rest.v("1").transcribe_file(payload, opts)
dg_tx = DeepgramConverter(res.to_dict())

# Gerar WebVTT
with open("output.vtt", "w", encoding="utf-8") as out:
    out.write(webvtt(dg_tx))

# Gerar SRT
with open("output.srt", "w", encoding="utf-8") as out:
    out.write(srt(dg_tx))
```

### Formato WebVTT vs SRT

| Caracter√≠stica | WebVTT | SRT |
|----------------|--------|-----|
| **Sintaxe de Tempo** | `HH:MM:SS.mmm` | `HH:MM:SS,mmm` |
| **Tags de Speaker** | `<v Speaker N>` nativo | Embutido no texto |
| **Melhor Para** | HTML5 `<track>` + CSS styling | Editores offline, players legados |
| **Suporte Web** | Excelente | Bom |
| **Styling** | Via `::cue` CSS | Limitado |

### Exemplo WebVTT Manual

```python
from datetime import timedelta

def format_timestamp(seconds: float, format_type="vtt") -> str:
    """Converte float seconds para timestamp"""
    td = timedelta(seconds=seconds)
    hours, remainder = divmod(td.seconds, 3600)
    minutes, seconds = divmod(remainder, 60)
    milliseconds = int(td.microseconds / 1000)

    if format_type == "srt":
        return f"{hours:02}:{minutes:02}:{seconds:02},{milliseconds:03}"
    else:  # WebVTT
        return f"{hours:02}:{minutes:02}:{seconds:02}.{milliseconds:03}"

def write_webvtt_file(utterances, output_file="output.vtt"):
    """Gera arquivo WebVTT"""
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("WEBVTT\n\n")
        for utt in utterances:
            start = format_timestamp(utt['start'], "vtt")
            end = format_timestamp(utt['end'], "vtt")
            speaker = utt.get('speaker', 0)
            transcript = utt['transcript']

            f.write(f"{start} --> {end}\n")
            f.write(f"<v Speaker {speaker}>{transcript}\n\n")
    
    print(f"‚úì WebVTT file saved as: {output_file}")
```

## üé® Sincroniza√ß√£o de √Åudio em React

### Problema
Sincronizar transcri√ß√£o visual com √°udio reproduzindo em tempo real, mantendo 60fps em dispositivos de baixa pot√™ncia.

### Solu√ß√£o: Usar Refs (N√£o State)

**‚ùå Approach Ruim (State):**
```javascript
// N√ÉO FAZER ISSO
const [currentTime, setCurrentTime] = useState(0);

useEffect(() => {
  const onTimeUpdate = () => {
    setCurrentTime(playerRef.current.currentTime); // Trigger render!
  };
  playerRef.current.addEventListener("timeupdate", onTimeUpdate);
}, []);
```

**Problema:** `timeupdate` dispara 4-66Hz, causando renders frequentes e FPS baixo.

**‚úÖ Approach Bom (Refs + DOM Direto):**
```javascript
const Transcript = ({ transcript }) => {
  const playerRef = useRef(null);
  const wordsRef = useRef(null);

  useEffect(() => {
    const onTimeUpdate = () => {
      const currentTime = playerRef.current.currentTime;
      
      // Encontrar palavra ativa
      const activeWordIndex = transcript.words.findIndex(
        word => word.startTime <= currentTime && word.endTime > currentTime
      );
      
      // Modificar DOM diretamente (fora do React render)
      if (activeWordIndex !== -1) {
        const wordElement = wordsRef.current.childNodes[activeWordIndex];
        
        // Remover highlight anterior
        wordsRef.current.querySelectorAll('.active-word')
          .forEach(el => el.classList.remove('active-word'));
        
        // Adicionar novo highlight
        wordElement.classList.add('active-word');
      }
    };
    
    playerRef.current.addEventListener("timeupdate", onTimeUpdate);
    return () => playerRef.current.removeEventListener("timeupdate", onTimeUpdate);
  }, [transcript]);

  return (
    <div>
      <span ref={wordsRef}>
        {transcript.words.map((word, i) => (
          <span key={i}>{word.text} </span>
        ))}
      </span>
      <audio controls src={audioSrc} ref={playerRef} />
    </div>
  );
};
```

### Performance: State vs Refs

| M√©todo | Event Handler Time | FPS | Responsividade |
|--------|-------------------|-----|----------------|
| **State** | >400ms | <15 FPS | Lenta, laggy |
| **Refs + DOM** | <1ms | 60 FPS | Suave, instant√¢nea |

## üîç Analytics e M√©tricas

### Talk Time por Speaker

```python
def calculate_talk_time_analytics(utterances):
    """Calcula m√©tricas detalhadas de fala"""
    analytics = {}
    
    for utt in utterances:
        speaker = utt['speaker']
        duration = utt['end'] - utt['start']
        word_count = len(utt['transcript'].split())
        
        if speaker not in analytics:
            analytics[speaker] = {
                'total_time': 0,
                'utterance_count': 0,
                'word_count': 0,
                'utterances': []
            }
        
        analytics[speaker]['total_time'] += duration
        analytics[speaker]['utterance_count'] += 1
        analytics[speaker]['word_count'] += word_count
        analytics[speaker]['utterances'].append(utt)
    
    # Calcular m√©tricas derivadas
    for speaker, data in analytics.items():
        data['avg_utterance_duration'] = (
            data['total_time'] / data['utterance_count']
        )
        data['words_per_minute'] = (
            data['word_count'] / (data['total_time'] / 60)
        )
    
    return analytics
```

### Detec√ß√£o de Interrup√ß√µes

```python
def detect_interruptions(utterances, overlap_threshold=0.2):
    """Detecta quando speakers se interrompem"""
    interruptions = []
    
    for i in range(len(utterances) - 1):
        current = utterances[i]
        next_utt = utterances[i + 1]
        
        # Verifica sobreposi√ß√£o
        if current['end'] > next_utt['start']:
            overlap = current['end'] - next_utt['start']
            
            if overlap > overlap_threshold:
                interruptions.append({
                    'speaker_interrupted': current['speaker'],
                    'speaker_interrupting': next_utt['speaker'],
                    'timestamp': next_utt['start'],
                    'overlap_duration': overlap
                })
    
    return interruptions
```

## üîê Best Practices de Produ√ß√£o

### 1. Escolha de Canal vs Diarization

```python
# Para call center com 2 canais separados
opts = PrerecordedOptions(
    channels=2,
    model="nova-3"
)

# Para reuni√£o com m√∫ltiplos falantes em 1 canal
opts = PrerecordedOptions(
    diarize=True,
    model="nova-3"
)
```

### 2. Smoothing de Utterances

```python
def smooth_utterances(utterances, min_gap=0.1):
    """Remove gaps muito pequenos entre utterances"""
    smoothed = []
    
    for i, utt in enumerate(utterances):
        if i > 0:
            prev = smoothed[-1]
            gap = utt['start'] - prev['end']
            
            if gap < min_gap and utt['speaker'] == prev['speaker']:
                # Mesclar com anterior
                prev['end'] = utt['end']
                prev['transcript'] += ' ' + utt['transcript']
                continue
        
        smoothed.append(utt.copy())
    
    return smoothed
```

### 3. Schema de Dados Normalizado

```python
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class Word:
    text: str
    start: float
    end: float
    confidence: float
    speaker: int

@dataclass
class Turn:
    id: str
    start: float
    end: float
    speaker: int
    text: str
    words: List[Word]
    channel: int
    confidence: float

def normalize_deepgram_response(response):
    """Converte resposta Deepgram em schema normalizado"""
    turns = []
    
    for utt in response['results']['utterances']:
        words = [
            Word(
                text=w['word'],
                start=w['start'],
                end=w['end'],
                confidence=w['confidence'],
                speaker=w['speaker']
            )
            for w in utt['words']
        ]
        
        turn = Turn(
            id=utt['id'],
            start=utt['start'],
            end=utt['end'],
            speaker=utt['speaker'],
            text=utt['transcript'],
            words=words,
            channel=utt['channel'],
            confidence=utt['confidence']
        )
        
        turns.append(turn)
    
    return turns
```

## üöÄ Recursos Avan√ßados

### Detec√ß√£o Autom√°tica de Idioma

```python
opts = PrerecordedOptions(
    detect_language=True,
    model="nova-3"
)
```

### Reda√ß√£o de PII (Informa√ß√µes Pessoais)

```python
opts = PrerecordedOptions(
    redact=["pii"],  # Remove informa√ß√µes pessoais
    model="nova-3"
)
```

### Filtro de Profanidade

```python
opts = PrerecordedOptions(
    profanity_filter=True,
    model="nova-3"
)
```

## üìä Monitoramento e Logging

### Estrutura de Log Recomendada

```python
import logging
from datetime import datetime

def log_transcription_job(
    job_id: str,
    audio_duration: float,
    speaker_count: int,
    word_count: int,
    processing_time: float
):
    """Log de job de transcri√ß√£o"""
    logger = logging.getLogger('deepgram_jobs')
    
    logger.info({
        'job_id': job_id,
        'timestamp': datetime.now().isoformat(),
        'audio_duration_seconds': audio_duration,
        'speaker_count': speaker_count,
        'word_count': word_count,
        'processing_time_seconds': processing_time,
        'words_per_second': word_count / audio_duration if audio_duration > 0 else 0
    })
```

## üîó Links √öteis

### Documenta√ß√£o Oficial
- [Deepgram Getting Started](https://developers.deepgram.com/docs/stt/getting-started)
- [Pre-recorded Audio Guide](https://developers.deepgram.com/docs/pre-recorded-audio)
- [Live Streaming Audio](https://developers.deepgram.com/docs/live-streaming-audio)
- [Speaker Diarization](https://developers.deepgram.com/docs/diarization)
- [Utterances Feature](https://developers.deepgram.com/docs/utterances)

### SDKs
- [Python SDK](https://github.com/deepgram/deepgram-python-sdk)
- [JavaScript SDK](https://github.com/deepgram/deepgram-js-sdk)
- [Go SDK](https://github.com/deepgram/deepgram-go-sdk)

### Helpers
- [deepgram-captions (Python)](https://github.com/deepgram/deepgram-python-captions)

### Comunidade
- [Deepgram Community Forum](https://community.deepgram.com/)
- [GitHub Discussions](https://github.com/orgs/deepgram/discussions)

## üí° Exemplos de Projetos

### 1. Player de V√≠deo com Legendas por Speaker
```
Funcionalidades:
- Carrega v√≠deo + WebVTT
- Cores diferentes por speaker
- Atualiza√ß√£o em tempo real
- Busca por palavra-chave
```

### 2. Meeting Assistant
```
Funcionalidades:
- Transcri√ß√£o de reuni√£o
- Sum√°rio por speaker
- Action items detectados
- Exporta√ß√£o de relat√≥rio
```

### 3. Compliance & QA
```
Funcionalidades:
- Verifica√ß√£o de cobertura de pol√≠ticas
- Reda√ß√£o autom√°tica de PII
- M√©tricas de qualidade
- Fila de revis√£o
```

### 4. Analytics Dashboard
```
Funcionalidades:
- Talk-time por speaker
- Taxa de interrup√ß√£o
- Pausa m√©dia
- T√≥picos detectados
```

---

**Conclus√£o**: Deepgram fornece uma API poderosa e flex√≠vel para transcri√ß√£o de √°udio com recursos avan√ßados de timestamps, diarization e an√°lise. Com a documenta√ß√£o e exemplos deste guia, voc√™ pode construir aplica√ß√µes de voz sofisticadas mantendo alta performance e precis√£o.

**Cr√©ditos:** $200 gr√°tis para come√ßar em [console.deepgram.com](https://console.deepgram.com/)
